# Foundation Model Configuration
# Frontier-1: Core Large Language Model

model_name: "frontier-1"
version: "1.0.0"
release_date: "2025-Q2"

# Architecture Configuration
architecture:
  type: "transformer_decoder"
  variant: "mixture_of_experts_3.0"
  
  # Core Parameters
  total_parameters: "175B"
  shared_parameters: "120B"
  expert_parameters: "55B"
  
  # Model Dimensions
  hidden_size: 12288
  intermediate_size: 49152
  num_layers: 96
  num_attention_heads: 96
  num_key_value_heads: 8  # Multi-query attention
  
  # Context and Vocabulary
  max_sequence_length: 131072  # 128K tokens
  vocab_size: 100000
  
  # Expert Configuration
  num_experts: 64
  experts_per_token: 8
  expert_layers: [12, 24, 36, 48, 60, 72, 84, 96]
  expert_routing: "hierarchical_tree"
  
  # Attention Mechanisms
  attention_type: "multi_scale_attention"
  local_attention_window: 512
  global_attention_pattern: "sparse_dilated"
  memory_augmented: true
  external_memory_size: 1048576  # 1M tokens

# Training Configuration
training:
  # Dataset
  total_training_tokens: "15T"
  dataset_composition:
    web_crawl: 0.40
    code_repositories: 0.20
    books_literature: 0.15
    scientific_papers: 0.10
    business_data: 0.08
    multimodal_pairs: 0.07
  
  # Training Hyperparameters
  learning_rate: 1.5e-4
  batch_size: 2048
  gradient_accumulation_steps: 8
  warmup_steps: 5000
  max_steps: 500000
  
  # Optimization
  optimizer: "adamw"
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  epsilon: 1e-8
  
  # Regularization
  dropout: 0.1
  attention_dropout: 0.1
  expert_dropout: 0.05
  
  # Mixed Precision
  precision: "bf16"
  gradient_checkpointing: true
  
  # Advanced Training Techniques
  constitutional_training: true
  rlhf_enabled: true
  federated_learning: false  # Enabled post-training

# Model Specialization Modules
specialized_modules:
  business_operations:
    parameters: "22B"
    fine_tuning_tokens: "500B"
    specialization_layers: 24
    domain_vocabulary_size: 50000
    
  web_development:
    parameters: "28B"
    fine_tuning_tokens: "800B"
    specialization_layers: 32
    code_vocabulary_size: 75000
    
  marketing_creative:
    parameters: "25B"
    fine_tuning_tokens: "600B"
    specialization_layers: 28
    creative_vocabulary_size: 60000

# Multimodal Integration
multimodal:
  vision_encoder:
    architecture: "vision_transformer_2025"
    parameters: "12B"
    input_resolution: [1024, 4096]
    patch_size: 16
    
  audio_encoder:
    architecture: "whisper_v3_enhanced"
    parameters: "3B"
    sample_rate: 48000
    languages: 150
    
  cross_modal_fusion:
    parameters: "8B"
    shared_embedding_dim: 4096
    fusion_layers: 12

# Safety and Alignment
safety:
  constitutional_ai: true
  content_filtering: true
  bias_mitigation: true
  privacy_protection: true
  
  safety_classifiers:
    - harmful_content
    - bias_detection
    - factual_accuracy
    - privacy_violation
    
  alignment_techniques:
    - constitutional_training
    - rlhf_preference_learning
    - red_team_testing
    - adversarial_training

# Performance Targets
performance:
  inference_latency:
    first_token: "100ms"
    subsequent_tokens: "20ms"
    
  throughput:
    tokens_per_second: 1000000
    concurrent_users: 100000
    
  quality_metrics:
    perplexity: "<10.0"
    bleu_score: ">0.85"
    human_preference: ">90%"

# Hardware Requirements
hardware:
  training:
    gpu_type: "H200_SXM"
    gpu_count: 2048
    memory_per_gpu: "141GB"
    interconnect: "NVLink_4.0"
    
  inference:
    gpu_type: "H100_SXM5"
    gpu_count: 8
    memory_per_gpu: "80GB"
    batch_size: 64
