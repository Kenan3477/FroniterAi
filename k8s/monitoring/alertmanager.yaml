# AlertManager Configuration for Frontier Production
# Complete alerting setup with multiple notification channels

apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'localhost:587'
      smtp_from: 'alerts@frontier.com'
      resolve_timeout: 5m

    templates:
    - '/etc/alertmanager/templates/*.tmpl'

    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'
      routes:
      - match:
          severity: critical
        receiver: 'critical-alerts'
        group_wait: 5s
        repeat_interval: 5m
      - match:
          severity: warning
        receiver: 'warning-alerts'
        repeat_interval: 30m
      - match:
          alertname: DeadMansSwitch
        receiver: 'deadmansswitch'

    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'cluster', 'service']

    receivers:
    - name: 'web.hook'
      webhook_configs:
      - url: 'http://webhook-service:5000/alerts'
        send_resolved: true

    - name: 'critical-alerts'
      email_configs:
      - to: 'ops-team@frontier.com'
        subject: 'CRITICAL: {{ .GroupLabels.alertname }} in {{ .GroupLabels.cluster }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Details:
          {{ range .Labels.SortedPairs }} - {{ .Name }} = {{ .Value }}
          {{ end }}
          {{ end }}
      slack_configs:
      - api_url: 'YOUR_SLACK_WEBHOOK_URL'
        channel: '#alerts-critical'
        title: 'Critical Alert in Frontier Production'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Severity:* {{ .Labels.severity }}
          {{ end }}

    - name: 'warning-alerts'
      email_configs:
      - to: 'dev-team@frontier.com'
        subject: 'WARNING: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          {{ end }}

    - name: 'deadmansswitch'
      webhook_configs:
      - url: 'https://deadmansswitch.com/ping/YOUR_UUID'
        send_resolved: false

---
# Prometheus Alert Rules
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-rules
  namespace: monitoring
data:
  frontier-api.yml: |
    groups:
    - name: frontier-api
      rules:
      # High error rate
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{job="frontier-api",status=~"5.."}[5m]))
            /
            sum(rate(http_requests_total{job="frontier-api"}[5m]))
          ) * 100 > 5
        for: 5m
        labels:
          severity: critical
          service: frontier-api
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }}% for the last 5 minutes"

      # High response time
      - alert: HighResponseTime
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket{job="frontier-api"}[5m])) by (le)
          ) > 0.5
        for: 5m
        labels:
          severity: warning
          service: frontier-api
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }}s"

      # High CPU usage
      - alert: HighCPUUsage
        expr: |
          sum(rate(container_cpu_usage_seconds_total{pod=~"frontier-api-.*"}[5m])) by (pod) * 100 > 80
        for: 10m
        labels:
          severity: warning
          service: frontier-api
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is {{ $value }}% for pod {{ $labels.pod }}"

      # High memory usage
      - alert: HighMemoryUsage
        expr: |
          (
            sum(container_memory_working_set_bytes{pod=~"frontier-api-.*"}) by (pod)
            /
            sum(container_spec_memory_limit_bytes{pod=~"frontier-api-.*"}) by (pod)
          ) * 100 > 90
        for: 5m
        labels:
          severity: critical
          service: frontier-api
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is {{ $value }}% for pod {{ $labels.pod }}"

      # Pod restart frequency
      - alert: PodRestartingFrequently
        expr: |
          increase(kube_pod_container_status_restarts_total{pod=~"frontier-api-.*"}[1h]) > 3
        for: 5m
        labels:
          severity: warning
          service: frontier-api
        annotations:
          summary: "Pod restarting frequently"
          description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last hour"

      # Database connection issues
      - alert: DatabaseConnectionIssues
        expr: |
          sum(rate(database_connections_failed_total{job="frontier-api"}[5m])) > 0.1
        for: 2m
        labels:
          severity: critical
          service: database
        annotations:
          summary: "Database connection failures detected"
          description: "Database connection failure rate is {{ $value }} per second"

      # Cache miss rate
      - alert: HighCacheMissRate
        expr: |
          (
            sum(rate(cache_misses_total{job="frontier-api"}[5m]))
            /
            sum(rate(cache_requests_total{job="frontier-api"}[5m]))
          ) * 100 > 50
        for: 10m
        labels:
          severity: warning
          service: cache
        annotations:
          summary: "High cache miss rate"
          description: "Cache miss rate is {{ $value }}% for the last 5 minutes"

  infrastructure.yml: |
    groups:
    - name: infrastructure
      rules:
      # Node down
      - alert: NodeDown
        expr: up{job="kubernetes-nodes"} == 0
        for: 5m
        labels:
          severity: critical
          service: infrastructure
        annotations:
          summary: "Kubernetes node is down"
          description: "Node {{ $labels.instance }} has been down for more than 5 minutes"

      # High node CPU
      - alert: NodeHighCPU
        expr: |
          (
            100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
          ) > 85
        for: 10m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "High CPU usage on node"
          description: "Node {{ $labels.instance }} CPU usage is {{ $value }}%"

      # High node memory
      - alert: NodeHighMemory
        expr: |
          (
            (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes)
            /
            node_memory_MemTotal_bytes
          ) * 100 > 85
        for: 10m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "High memory usage on node"
          description: "Node {{ $labels.instance }} memory usage is {{ $value }}%"

      # Disk space low
      - alert: NodeDiskSpaceLow
        expr: |
          (
            (node_filesystem_size_bytes{fstype!="tmpfs"} - node_filesystem_free_bytes{fstype!="tmpfs"})
            /
            node_filesystem_size_bytes{fstype!="tmpfs"}
          ) * 100 > 85
        for: 5m
        labels:
          severity: warning
          service: infrastructure
        annotations:
          summary: "Low disk space on node"
          description: "Node {{ $labels.instance }} disk usage is {{ $value }}% on {{ $labels.mountpoint }}"

  deadmansswitch.yml: |
    groups:
    - name: deadmansswitch
      rules:
      - alert: DeadMansSwitch
        expr: vector(1)
        labels:
          severity: none
        annotations:
          summary: "Alerting DeadMansSwitch"
          description: "This is a DeadMansSwitch meant to ensure that the entire alerting pipeline is functional."

---
# AlertManager Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
  labels:
    app: alertmanager
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        fsGroup: 65534
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.25.0
        imagePullPolicy: IfNotPresent
        args:
        - --config.file=/etc/alertmanager/alertmanager.yml
        - --storage.path=/alertmanager
        - --web.external-url=http://alertmanager:9093
        - --cluster.advertise-address=$(POD_IP):9094
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        ports:
        - containerPort: 9093
          name: web
        - containerPort: 9094
          name: mesh
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9093
          initialDelaySeconds: 30
          periodSeconds: 15
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9093
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 4
          failureThreshold: 3
        volumeMounts:
        - name: alertmanager-config
          mountPath: /etc/alertmanager
        - name: alertmanager-storage
          mountPath: /alertmanager
      volumes:
      - name: alertmanager-config
        configMap:
          name: alertmanager-config
      - name: alertmanager-storage
        emptyDir: {}

---
# AlertManager Service
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: monitoring
  labels:
    app: alertmanager
spec:
  type: ClusterIP
  ports:
  - port: 9093
    targetPort: 9093
    name: web
  selector:
    app: alertmanager
